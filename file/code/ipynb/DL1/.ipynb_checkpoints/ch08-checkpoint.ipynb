{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#作業フォルダ移動（ch08）\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir(\"../../deep-learning-from-scratch-master/ch08\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train_deepnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading train-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-images-idx3-ubyte.gz ... \n",
      "Done\n",
      "Downloading t10k-labels-idx1-ubyte.gz ... \n",
      "Done\n",
      "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
      "Done\n",
      "Creating pickle file ...\n",
      "Done!\n",
      "train loss:2.3234431017741906\n",
      "=== epoch:1, train acc:0.056, test acc:0.079 ===\n",
      "train loss:2.2711647261951295\n",
      "train loss:2.3145956653607698\n",
      "train loss:2.3305383545031946\n",
      "train loss:2.2797815165321125\n",
      "train loss:2.2789041054510464\n",
      "train loss:2.272748396658631\n",
      "train loss:2.3092637243571104\n",
      "train loss:2.26991532468695\n",
      "train loss:2.2390379536829776\n",
      "train loss:2.2274691881306983\n",
      "train loss:2.264266279807557\n",
      "train loss:2.253054784648626\n",
      "train loss:2.229616092694176\n",
      "train loss:2.2025221836298776\n",
      "train loss:2.2172656387735206\n",
      "train loss:2.1522205229803792\n",
      "train loss:2.2077779446538224\n",
      "train loss:2.228293887946739\n",
      "train loss:2.168257785726867\n",
      "train loss:2.071311167613115\n",
      "train loss:2.127623571960007\n",
      "train loss:2.0801817555654365\n",
      "train loss:2.0684126261228504\n",
      "train loss:2.0370980453295298\n",
      "train loss:1.8666762470299296\n",
      "train loss:1.9266946070208641\n",
      "train loss:2.059492939119779\n",
      "train loss:2.057252007713184\n",
      "train loss:1.9452873943496454\n",
      "train loss:1.982984810192812\n",
      "train loss:1.9049535024074944\n",
      "train loss:1.9384221699582767\n",
      "train loss:1.9345545493533831\n",
      "train loss:1.8161390769741825\n",
      "train loss:1.7838311804836997\n",
      "train loss:1.806935776787465\n",
      "train loss:1.8261045319165703\n",
      "train loss:1.8328124238646342\n",
      "train loss:1.7433453023025538\n",
      "train loss:1.7899694780379292\n",
      "train loss:1.6776772202450823\n",
      "train loss:1.599699088819157\n",
      "train loss:1.7674833011594695\n",
      "train loss:1.6531922457781787\n",
      "train loss:1.6324032014408052\n",
      "train loss:1.5463441471566879\n",
      "train loss:1.8004586134475398\n",
      "train loss:1.764459084045445\n",
      "train loss:1.470882690041093\n",
      "train loss:1.709800720134923\n",
      "train loss:1.6013824883567798\n",
      "train loss:1.7088538051740338\n",
      "train loss:1.5723873380943574\n",
      "train loss:1.6174197156028827\n",
      "train loss:1.6483560618734674\n",
      "train loss:1.5049483430479982\n",
      "train loss:1.6300106529641878\n",
      "train loss:1.7473314071993642\n",
      "train loss:1.40123011080619\n",
      "train loss:1.487888508181543\n",
      "train loss:1.648832740079405\n",
      "train loss:1.3313825433831779\n",
      "train loss:1.634566241398021\n",
      "train loss:1.5534534702354221\n",
      "train loss:1.6334259241731053\n",
      "train loss:1.427994059391266\n",
      "train loss:1.6133486190477313\n",
      "train loss:1.5514601571520452\n",
      "train loss:1.4506840441472608\n",
      "train loss:1.6004448503731399\n",
      "train loss:1.6423221860332347\n",
      "train loss:1.3335126714675924\n",
      "train loss:1.4914663327728797\n",
      "train loss:1.5795928129622163\n",
      "train loss:1.6002250846001247\n",
      "train loss:1.711573242523944\n",
      "train loss:1.44459980879612\n",
      "train loss:1.7026492274597749\n",
      "train loss:1.6315063986253733\n",
      "train loss:1.4909896144696269\n",
      "train loss:1.3919728820262807\n",
      "train loss:1.497339448493399\n",
      "train loss:1.5093342621832517\n",
      "train loss:1.6083222583639754\n",
      "train loss:1.5387705718596327\n",
      "train loss:1.5872303627112783\n",
      "train loss:1.6051791468444083\n",
      "train loss:1.4340420498649977\n",
      "train loss:1.4486514252342615\n",
      "train loss:1.5025907941216992\n",
      "train loss:1.4018063983177316\n",
      "train loss:1.4468611100572133\n",
      "train loss:1.3543843460860887\n",
      "train loss:1.5172027841457443\n",
      "train loss:1.4667718101019096\n",
      "train loss:1.279989976102716\n",
      "train loss:1.781100946360401\n",
      "train loss:1.6146495381744834\n",
      "train loss:1.5478568180590357\n",
      "train loss:1.1300165584676853\n",
      "train loss:1.3753539524240455\n",
      "train loss:1.517392249029622\n",
      "train loss:1.6614394901192355\n",
      "train loss:1.3758186435541604\n",
      "train loss:1.511857870233109\n",
      "train loss:1.4147896670281546\n",
      "train loss:1.556310170612608\n",
      "train loss:1.2843692198064673\n",
      "train loss:1.4602307571735864\n",
      "train loss:1.4641006942468735\n",
      "train loss:1.5204488162869323\n",
      "train loss:1.2589384328079747\n",
      "train loss:1.3926572546608107\n",
      "train loss:1.3258012799162318\n",
      "train loss:1.4022563578160367\n",
      "train loss:1.4396126584261841\n",
      "train loss:1.4388303889101908\n",
      "train loss:1.4080948773162867\n",
      "train loss:1.4001315228943794\n",
      "train loss:1.2732349888141024\n",
      "train loss:1.2571735702299525\n",
      "train loss:1.3299968459457887\n",
      "train loss:1.5588738168808738\n",
      "train loss:1.3369919611054772\n",
      "train loss:1.515941164035761\n",
      "train loss:1.4032287003684685\n",
      "train loss:1.3117503238460235\n",
      "train loss:1.4518632925109154\n",
      "train loss:1.4240133906188757\n",
      "train loss:1.3264176001641539\n",
      "train loss:1.4122302994880578\n",
      "train loss:1.4533865336434522\n",
      "train loss:1.262975820764014\n",
      "train loss:1.4805592480905458\n",
      "train loss:1.343320349699749\n",
      "train loss:1.482878877303707\n",
      "train loss:1.6040180340586778\n",
      "train loss:1.3938841169706384\n",
      "train loss:1.3370265061940487\n",
      "train loss:1.2333358709466844\n",
      "train loss:1.3861974209684882\n",
      "train loss:1.3989934607655024\n",
      "train loss:1.3817005318874658\n",
      "train loss:1.4835484123729359\n",
      "train loss:1.236308474364673\n",
      "train loss:1.295580933565568\n",
      "train loss:1.3759083238882277\n",
      "train loss:1.0966235796956927\n",
      "train loss:1.5740422041097355\n",
      "train loss:1.466289885098882\n",
      "train loss:1.551813571549748\n",
      "train loss:1.2834412249437037\n",
      "train loss:1.495161802759859\n",
      "train loss:1.3977386086068484\n",
      "train loss:1.2952908878441607\n",
      "train loss:1.29394693613368\n",
      "train loss:1.2664966088683687\n",
      "train loss:1.2817701234827714\n",
      "train loss:1.2711477464478487\n",
      "train loss:1.4093623415281358\n",
      "train loss:1.2416046162571424\n",
      "train loss:1.3819829313193748\n",
      "train loss:1.2866061824655892\n",
      "train loss:1.359580596185552\n",
      "train loss:1.3838636150441022\n",
      "train loss:1.3552393892038848\n",
      "train loss:1.350857376275978\n",
      "train loss:1.2455901420973055\n",
      "train loss:1.2007082796384017\n",
      "train loss:1.2119656235984189\n",
      "train loss:1.3094936967599964\n",
      "train loss:1.3307230963363514\n",
      "train loss:1.2536706297405915\n",
      "train loss:1.3592344282097306\n",
      "train loss:1.329300752881388\n",
      "train loss:1.2435292758739864\n",
      "train loss:1.2881580449511725\n",
      "train loss:1.3243013338290195\n",
      "train loss:1.2292664949128052\n",
      "train loss:1.348328590076703\n",
      "train loss:1.2705782461168487\n",
      "train loss:1.1862140366480076\n",
      "train loss:1.1654211585771967\n",
      "train loss:1.4597045513705973\n",
      "train loss:1.116455105705484\n",
      "train loss:1.3025092715328819\n",
      "train loss:1.3342954359464432\n",
      "train loss:1.3264443689749705\n",
      "train loss:1.24615315424452\n",
      "train loss:1.2306556825997768\n",
      "train loss:1.2772248926641627\n",
      "train loss:1.2321224855421955\n",
      "train loss:1.1338080405260598\n",
      "train loss:1.150394664872535\n",
      "train loss:1.285325190409316\n",
      "train loss:1.1836632948988024\n",
      "train loss:1.184513524407695\n",
      "train loss:0.9007771363624566\n",
      "train loss:1.2218404528099722\n",
      "train loss:1.2689750660894814\n",
      "train loss:1.2768699314919265\n",
      "train loss:1.2975127507847448\n",
      "train loss:1.2520049012306342\n",
      "train loss:1.2431883143595566\n",
      "train loss:1.3269894825124822\n",
      "train loss:1.221460214162461\n",
      "train loss:1.1035038223841236\n",
      "train loss:1.1141814594946553\n",
      "train loss:1.1042487958516698\n",
      "train loss:1.2491538169148335\n",
      "train loss:1.322387569439559\n",
      "train loss:1.1947910770449228\n",
      "train loss:1.3146356834128006\n",
      "train loss:1.2605930185519154\n",
      "train loss:1.293615535447944\n",
      "train loss:1.2762756467131384\n",
      "train loss:1.4587051491522656\n",
      "train loss:1.3645799619335293\n",
      "train loss:1.3602675739847123\n",
      "train loss:1.2931613963088262\n",
      "train loss:1.3504039040259377\n",
      "train loss:1.2402942360157998\n",
      "train loss:1.155595230596955\n",
      "train loss:1.3646163484244398\n",
      "train loss:1.3626759094253216\n",
      "train loss:1.1274941398323592\n",
      "train loss:1.3186516282704823\n",
      "train loss:1.2798790585148987\n",
      "train loss:1.3904978504322167\n",
      "train loss:1.1770883364536537\n",
      "train loss:1.3667498042441262\n",
      "train loss:1.2828974911422215\n",
      "train loss:1.381855667296733\n",
      "train loss:1.2224946547649806\n",
      "train loss:1.303538326111346\n",
      "train loss:1.101790276751815\n",
      "train loss:1.2626680149928207\n",
      "train loss:1.124209043908498\n",
      "train loss:1.4415321852172645\n",
      "train loss:0.9473073570006071\n",
      "train loss:1.3031912041322657\n",
      "train loss:1.3350021466419861\n",
      "train loss:1.5029648533942517\n",
      "train loss:1.3348735794490763\n",
      "train loss:1.3254624690540384\n",
      "train loss:1.2368594142945044\n",
      "train loss:1.3330309829080005\n",
      "train loss:1.179447322351306\n",
      "train loss:1.171007371869502\n",
      "train loss:1.006382622593932\n",
      "train loss:1.1516484817707409\n",
      "train loss:1.286544742907026\n",
      "train loss:1.185761785040156\n",
      "train loss:1.2634467483143281\n",
      "train loss:1.19667211696574\n",
      "train loss:1.207400111173332\n",
      "train loss:1.1368761095201234\n",
      "train loss:1.3105156722987694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.2642408790720898\n",
      "train loss:1.1643639721138688\n",
      "train loss:1.147138380365014\n",
      "train loss:1.0749198611064328\n",
      "train loss:1.1244775324134326\n",
      "train loss:1.1008414316655044\n",
      "train loss:1.269022321003686\n",
      "train loss:1.2057068397907007\n",
      "train loss:1.1368640827261123\n",
      "train loss:1.113319370610366\n",
      "train loss:1.1631393028760233\n",
      "train loss:1.2220159705458404\n",
      "train loss:1.0941534122759393\n",
      "train loss:1.3634582745971964\n",
      "train loss:1.3694888614802716\n",
      "train loss:1.1148101484513804\n",
      "train loss:1.221860341629668\n",
      "train loss:1.386738472319562\n",
      "train loss:1.1148099366452524\n",
      "train loss:1.3394720207381132\n",
      "train loss:1.2363946852769039\n",
      "train loss:1.1599383771436904\n",
      "train loss:1.2087117936612715\n",
      "train loss:1.2699642469819608\n",
      "train loss:1.1408263608449176\n",
      "train loss:1.1518908366908123\n",
      "train loss:1.1963927643521841\n",
      "train loss:0.9799157130534296\n",
      "train loss:1.1074830729647653\n",
      "train loss:1.223637040934254\n",
      "train loss:1.3085922449829803\n",
      "train loss:1.2622249491444508\n",
      "train loss:1.2527952492341174\n",
      "train loss:1.397297118606824\n",
      "train loss:1.0798832190374197\n",
      "train loss:1.2362536597488758\n",
      "train loss:1.2193148304359231\n",
      "train loss:1.277536598772392\n",
      "train loss:0.943873461524406\n",
      "train loss:1.0164282393615642\n",
      "train loss:1.2348268931061606\n",
      "train loss:1.2266063334043806\n",
      "train loss:1.1097791336612866\n",
      "train loss:1.1276704711440388\n",
      "train loss:1.0232467231130657\n",
      "train loss:1.1943382211179765\n",
      "train loss:1.1415648645471048\n",
      "train loss:1.2223667571658907\n",
      "train loss:1.2715803646337116\n",
      "train loss:1.2865240383153798\n",
      "train loss:1.0919270976814743\n",
      "train loss:1.1649186291814673\n",
      "train loss:1.264836580730131\n",
      "train loss:1.1457769599257628\n",
      "train loss:1.0340237622860884\n",
      "train loss:1.2657134490921451\n",
      "train loss:1.3336602292575392\n",
      "train loss:1.3618358915546418\n",
      "train loss:1.2984107912084824\n",
      "train loss:1.124040936174884\n",
      "train loss:1.0434326045557227\n",
      "train loss:1.0488981850642156\n",
      "train loss:1.3483779621374312\n",
      "train loss:0.9509950449953215\n",
      "train loss:1.2282160501124386\n",
      "train loss:1.1766638955078448\n",
      "train loss:1.1958917589419407\n",
      "train loss:1.1521848177992422\n",
      "train loss:1.2285230064387542\n",
      "train loss:1.0767729822573298\n",
      "train loss:1.1122167187484282\n",
      "train loss:1.05411306659245\n",
      "train loss:1.1110866299536384\n",
      "train loss:1.2567063031867565\n",
      "train loss:1.1133028666530287\n",
      "train loss:1.0815217645235888\n",
      "train loss:1.1366723498080682\n",
      "train loss:1.2201914253498543\n",
      "train loss:0.9957218177062007\n",
      "train loss:1.311715160084833\n",
      "train loss:1.1172864741537045\n",
      "train loss:1.2275100876429874\n",
      "train loss:1.112750878958566\n",
      "train loss:1.1502773805754203\n",
      "train loss:1.2371772764464466\n",
      "train loss:1.0665699784752345\n",
      "train loss:0.9650264343236225\n",
      "train loss:1.13552466505905\n",
      "train loss:1.1596495696294402\n",
      "train loss:1.0435321680888852\n",
      "train loss:1.0415752532045892\n",
      "train loss:1.1271658796809922\n",
      "train loss:0.987190406914324\n",
      "train loss:1.1217011257841327\n",
      "train loss:1.311875703442683\n",
      "train loss:1.1695700775527744\n",
      "train loss:0.9901008941785012\n",
      "train loss:1.244716418462899\n",
      "train loss:1.0813237555875133\n",
      "train loss:1.2114992587613667\n",
      "train loss:1.1686795650494952\n",
      "train loss:1.2807741675537212\n",
      "train loss:0.9971911971929714\n",
      "train loss:1.1604428834373115\n",
      "train loss:1.009065979444283\n",
      "train loss:1.1125800271902266\n",
      "train loss:0.9701336043000482\n",
      "train loss:1.1608254203177106\n",
      "train loss:1.147153909604959\n",
      "train loss:1.0986695653585192\n",
      "train loss:1.0697763469569244\n",
      "train loss:1.1851033619601794\n",
      "train loss:1.0645911404118569\n",
      "train loss:1.067019927083408\n",
      "train loss:1.1535665175598997\n",
      "train loss:1.1741964946419419\n",
      "train loss:1.1134082186287515\n",
      "train loss:1.062951093569966\n",
      "train loss:1.1516821422285792\n",
      "train loss:1.068863309666947\n",
      "train loss:1.1673782775077524\n",
      "train loss:1.2580850245515194\n",
      "train loss:1.1495048046383711\n",
      "train loss:1.0312010607398188\n",
      "train loss:1.2037803754630283\n",
      "train loss:1.1432534269327765\n",
      "train loss:1.241293457110408\n",
      "train loss:1.180622249873972\n",
      "train loss:1.109048768570015\n",
      "train loss:1.2776321016771723\n",
      "train loss:1.044986034386693\n",
      "train loss:1.0339968383035274\n",
      "train loss:1.0068843760647852\n",
      "train loss:1.087367832313663\n",
      "train loss:0.975403465820349\n",
      "train loss:1.1097957406442014\n",
      "train loss:1.0985173327055089\n",
      "train loss:1.1318449326572342\n",
      "train loss:1.2923219404392694\n",
      "train loss:0.9929097107834667\n",
      "train loss:1.191058186000525\n",
      "train loss:1.183659381750549\n",
      "train loss:1.0073432913404188\n",
      "train loss:1.0133616601898756\n",
      "train loss:1.2242624509558766\n",
      "train loss:1.2181545372334837\n",
      "train loss:1.151800324500009\n",
      "train loss:1.197374418168235\n",
      "train loss:1.22598233492182\n",
      "train loss:1.0520565671315187\n",
      "train loss:1.020949615920254\n",
      "train loss:0.9948706667590203\n",
      "train loss:1.118349380861283\n",
      "train loss:1.1245301392479563\n",
      "train loss:1.130626125746669\n",
      "train loss:1.1849337651545409\n",
      "train loss:1.1200580418300623\n",
      "train loss:1.0663641669923243\n",
      "train loss:1.191787001688463\n",
      "train loss:1.1738242353497932\n",
      "train loss:1.1273757705436847\n",
      "train loss:1.103808796122499\n",
      "train loss:1.093562143034712\n",
      "train loss:1.2370772216403148\n",
      "train loss:1.0384139480754346\n",
      "train loss:1.1665083067762594\n",
      "train loss:1.0777218533149164\n",
      "train loss:1.1616612579380863\n",
      "train loss:1.0420934793113616\n",
      "train loss:1.1898387522474467\n",
      "train loss:1.3518033274823553\n",
      "train loss:1.3694025626207829\n",
      "train loss:1.0246369759894451\n",
      "train loss:1.0463965595635507\n",
      "train loss:0.9061879083873283\n",
      "train loss:1.0486871352754654\n",
      "train loss:1.0602396512783507\n",
      "train loss:0.8444401332174902\n",
      "train loss:0.9968981179338776\n",
      "train loss:1.066410842589992\n",
      "train loss:1.193017647228296\n",
      "train loss:1.2288061802823267\n",
      "train loss:1.1327400187116372\n",
      "train loss:1.2545275293901545\n",
      "train loss:1.0131363143648713\n",
      "train loss:0.8791426860352611\n",
      "train loss:1.0876709968132323\n",
      "train loss:1.1831580031035736\n",
      "train loss:1.0524469646435946\n",
      "train loss:0.9282025058163085\n",
      "train loss:1.1597255529217314\n",
      "train loss:1.1221752776495615\n",
      "train loss:1.0112956503393637\n",
      "train loss:1.1352283652869666\n",
      "train loss:1.0845381078441434\n",
      "train loss:0.9843151125585698\n",
      "train loss:1.192122330295574\n",
      "train loss:1.0295496059651983\n",
      "train loss:1.189325850902326\n",
      "train loss:1.0005882143721814\n",
      "train loss:1.0073790340121662\n",
      "train loss:0.9381596263521296\n",
      "train loss:0.9908252825800993\n",
      "train loss:1.1079088878860184\n",
      "train loss:1.124721991246808\n",
      "train loss:0.9999626727131434\n",
      "train loss:0.9796031381012975\n",
      "train loss:1.0847487167196344\n",
      "train loss:1.1780902179323105\n",
      "train loss:1.2528831230006237\n",
      "train loss:1.23343690678637\n",
      "train loss:0.9782243206748008\n",
      "train loss:1.239569430750597\n",
      "train loss:1.0840690760978702\n",
      "train loss:1.0866535260888721\n",
      "train loss:1.065854799592914\n",
      "train loss:1.0421437868554646\n",
      "train loss:1.252467726595303\n",
      "train loss:0.7963061390487229\n",
      "train loss:0.8949465408729115\n",
      "train loss:0.9461526093359219\n",
      "train loss:0.9594055235064541\n",
      "train loss:1.2042177550139979\n",
      "train loss:0.9889337056314054\n",
      "train loss:0.8553059463549237\n",
      "train loss:0.966204043212676\n",
      "train loss:0.885944858003225\n",
      "train loss:1.1150363335330113\n",
      "train loss:0.992141161748203\n",
      "train loss:1.0441806231024802\n",
      "train loss:1.168244886796517\n",
      "train loss:1.1627367189759825\n",
      "train loss:1.1599800710044008\n",
      "train loss:1.107228638551846\n",
      "train loss:1.1297849888897873\n",
      "train loss:1.1311272202789864\n",
      "train loss:0.9991212425173882\n",
      "train loss:1.1380682073505952\n",
      "train loss:0.993246863962697\n",
      "train loss:0.8242303271023635\n",
      "train loss:1.2128862299990726\n",
      "train loss:1.035385565269805\n",
      "train loss:1.0767186798827089\n",
      "train loss:1.0194524391823467\n",
      "train loss:0.96222557108457\n",
      "train loss:1.0516406866989814\n",
      "train loss:1.0063598670347091\n",
      "train loss:0.9831579058454822\n",
      "train loss:0.8928868358189506\n",
      "train loss:1.087882236152127\n",
      "train loss:1.1506583641837873\n",
      "train loss:0.8383993646414747\n",
      "train loss:0.8897967128280923\n",
      "train loss:0.8996921154498939\n",
      "train loss:0.9286302980930756\n",
      "train loss:0.9038423948349026\n",
      "train loss:1.072210821012785\n",
      "train loss:1.1883992708580227\n",
      "train loss:1.0238301362145545\n",
      "train loss:1.0624059564569284\n",
      "train loss:1.140129813702255\n",
      "train loss:1.033731374994732\n",
      "train loss:1.2569890689494323\n",
      "train loss:1.282533416579433\n",
      "train loss:1.1616689008142693\n",
      "train loss:1.1191997071562374\n",
      "train loss:1.131328414721259\n",
      "train loss:1.1910379762772192\n",
      "train loss:1.1613609814308645\n",
      "train loss:1.2490082945904026\n",
      "train loss:1.228394924376913\n",
      "train loss:1.1313608505679402\n",
      "train loss:1.1733711667620503\n",
      "train loss:1.0497909691753577\n",
      "train loss:1.1970164251316289\n",
      "train loss:1.1350705737137783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.1178921780685556\n",
      "train loss:1.0902233053058206\n",
      "train loss:1.0945031322541117\n",
      "train loss:1.0129581455949994\n",
      "train loss:0.9968982958013303\n",
      "train loss:1.3350699178028373\n",
      "train loss:1.0479733063669054\n",
      "train loss:0.9781296576872169\n",
      "train loss:1.1381818141585984\n",
      "train loss:1.0306457101351052\n",
      "train loss:0.9923953078848399\n",
      "train loss:0.9278837979523796\n",
      "train loss:1.1133080780912352\n",
      "train loss:0.9684730922712197\n",
      "train loss:1.0511739378626128\n",
      "train loss:0.974734451198371\n",
      "train loss:0.9980487753380943\n",
      "train loss:1.246427745213695\n",
      "train loss:1.0551524554399918\n",
      "train loss:1.135242453068317\n",
      "train loss:1.1010605804625084\n",
      "train loss:1.018869566841783\n",
      "train loss:1.2240802019838553\n",
      "train loss:0.8066353840834003\n",
      "train loss:1.0592600047526186\n",
      "train loss:0.9938189275809693\n",
      "train loss:1.0157247829177922\n",
      "train loss:0.9263985790966399\n",
      "train loss:0.8101781683806935\n",
      "train loss:1.047504810733104\n",
      "train loss:1.2726724142076389\n",
      "train loss:1.0975175463819387\n",
      "train loss:1.0534494719460064\n",
      "train loss:1.1596176587227567\n",
      "train loss:1.046641988134788\n",
      "train loss:1.2019634027195192\n",
      "train loss:1.1224130374312289\n",
      "train loss:1.117358757958338\n",
      "train loss:1.259175138739648\n",
      "train loss:1.0682246543506633\n",
      "train loss:0.9219117765032416\n",
      "train loss:0.9365743820263313\n",
      "train loss:1.1199844377958315\n",
      "train loss:1.0454747243604015\n",
      "train loss:1.0537288494922308\n",
      "train loss:0.988963364738018\n",
      "train loss:1.130121699416062\n",
      "train loss:1.0909379033445052\n",
      "train loss:1.1017233706673273\n",
      "train loss:1.0948088809796717\n",
      "train loss:1.103247522721912\n",
      "train loss:0.9593813934512739\n",
      "train loss:1.0106945744400626\n",
      "train loss:1.00826288690481\n",
      "train loss:1.2878477000143058\n",
      "train loss:0.9645365279007084\n",
      "train loss:1.2425172396552393\n",
      "train loss:1.161270829264562\n",
      "train loss:1.1300601311627438\n",
      "train loss:1.1511120185348778\n",
      "train loss:1.1772828614079638\n",
      "train loss:1.043365909289354\n",
      "train loss:0.9533242251397399\n",
      "train loss:1.0906333308187335\n",
      "train loss:0.9811206232314446\n",
      "train loss:0.9075001996419634\n",
      "=== epoch:2, train acc:0.976, test acc:0.978 ===\n",
      "train loss:0.7754448826444643\n",
      "train loss:0.8933543382008001\n",
      "train loss:0.8491248489110648\n",
      "train loss:0.9693685170377817\n",
      "train loss:1.084971834892416\n",
      "train loss:1.0977157520171124\n",
      "train loss:1.1516246376459693\n",
      "train loss:1.2204879772137207\n",
      "train loss:1.2961877425324468\n",
      "train loss:0.9116301488279978\n",
      "train loss:0.9335463200751224\n",
      "train loss:1.0491475319660686\n",
      "train loss:1.0232820872887478\n",
      "train loss:1.035093534367904\n",
      "train loss:1.0183798969886206\n",
      "train loss:0.8709714376394213\n",
      "train loss:0.9785280389273165\n",
      "train loss:0.9875003084830013\n",
      "train loss:1.09926872390646\n",
      "train loss:1.203827076394048\n",
      "train loss:1.0129970890826954\n",
      "train loss:1.1259346321333665\n",
      "train loss:0.9968846975272824\n",
      "train loss:1.1267419092506374\n",
      "train loss:1.0138117725795235\n",
      "train loss:1.1682921629236829\n",
      "train loss:1.14931816634251\n",
      "train loss:1.1929581452334845\n",
      "train loss:0.7986414784203941\n",
      "train loss:1.1136559014566074\n",
      "train loss:1.0332696714424805\n",
      "train loss:0.9435185786456662\n",
      "train loss:1.0034583503982608\n",
      "train loss:1.0491309139006484\n",
      "train loss:0.9912665764524603\n",
      "train loss:1.1513575757197971\n",
      "train loss:1.168778490489181\n",
      "train loss:1.1527397411288889\n",
      "train loss:1.086590381478517\n",
      "train loss:1.1328617857443628\n",
      "train loss:0.9357151689317262\n",
      "train loss:0.902234568210361\n",
      "train loss:0.843666072411529\n",
      "train loss:0.9465365946643126\n",
      "train loss:1.0488605888359104\n",
      "train loss:0.8736317631908392\n",
      "train loss:0.872235426527904\n",
      "train loss:0.9217817539752895\n",
      "train loss:1.0299076570240189\n",
      "train loss:0.9734414489475334\n",
      "train loss:1.0073437278053794\n",
      "train loss:1.0430312768086762\n",
      "train loss:0.9740717390713368\n",
      "train loss:1.07677619181628\n",
      "train loss:1.09203402162582\n",
      "train loss:0.9088606314050618\n",
      "train loss:1.0920813999287287\n",
      "train loss:1.0242165402553918\n",
      "train loss:0.9585194211688214\n",
      "train loss:0.8481816411391931\n",
      "train loss:0.9607792326570282\n",
      "train loss:1.1507924930118898\n",
      "train loss:0.8758321453768068\n",
      "train loss:1.1238661587894379\n",
      "train loss:1.1201453072285366\n",
      "train loss:1.0296140729325247\n",
      "train loss:0.8913356155524441\n",
      "train loss:0.9697305757770962\n",
      "train loss:1.1164959362585556\n",
      "train loss:1.1112963170256398\n",
      "train loss:1.0090053456247774\n",
      "train loss:1.0458059397673116\n",
      "train loss:1.1186755776088706\n",
      "train loss:1.0567334521991478\n",
      "train loss:0.9502081574699429\n",
      "train loss:1.1084259067859734\n",
      "train loss:0.9937705661786334\n",
      "train loss:1.0095223936715867\n",
      "train loss:1.0029949833636358\n",
      "train loss:1.0487783192668165\n",
      "train loss:1.0544416219679063\n",
      "train loss:1.0559068857954481\n",
      "train loss:1.0854816356495027\n",
      "train loss:0.9286781188086861\n",
      "train loss:1.0826668654413745\n",
      "train loss:1.1683976529875688\n",
      "train loss:0.9378229708396237\n",
      "train loss:0.9571231264306305\n",
      "train loss:1.0068212288011982\n",
      "train loss:1.042832968523426\n",
      "train loss:1.1607821796107793\n",
      "train loss:1.076302075816059\n",
      "train loss:0.9200648582634406\n",
      "train loss:0.9638567293607997\n",
      "train loss:0.9996434021986\n",
      "train loss:1.0727708157951878\n",
      "train loss:0.9280753023113181\n",
      "train loss:0.9604617744328489\n",
      "train loss:0.9807723723662342\n",
      "train loss:1.1057422309426412\n",
      "train loss:1.0090066547578993\n",
      "train loss:1.053253149991762\n",
      "train loss:1.0591568035556034\n",
      "train loss:0.9789090841809103\n",
      "train loss:0.9605731839840864\n",
      "train loss:1.0508812446561107\n",
      "train loss:0.9986878569727381\n",
      "train loss:1.0050179982278946\n",
      "train loss:1.026425071439671\n",
      "train loss:1.0326992885302035\n",
      "train loss:0.9955541451120379\n",
      "train loss:1.0734247359102531\n",
      "train loss:1.015876741472805\n",
      "train loss:0.9907189602241894\n",
      "train loss:1.1191837942723701\n",
      "train loss:1.174582479612666\n",
      "train loss:1.0628037946560667\n",
      "train loss:0.9754758730233682\n",
      "train loss:1.0679665966354164\n",
      "train loss:1.0007934846520659\n",
      "train loss:0.9378860415626593\n",
      "train loss:1.1135733721570256\n",
      "train loss:0.9542532927484928\n",
      "train loss:1.1056389628786127\n",
      "train loss:1.077417355165118\n",
      "train loss:0.9937655027575606\n",
      "train loss:1.2180401261753262\n",
      "train loss:1.0538333465980778\n",
      "train loss:1.0444510965234661\n",
      "train loss:0.8870392757325339\n",
      "train loss:0.9959447939274105\n",
      "train loss:1.1290284673004605\n",
      "train loss:0.9184748888161118\n",
      "train loss:0.9801763092073397\n",
      "train loss:1.1227405641543\n",
      "train loss:1.1021259342163532\n",
      "train loss:1.2344659752008842\n",
      "train loss:1.1061948465462799\n",
      "train loss:1.168692245004197\n",
      "train loss:0.9097756667206959\n",
      "train loss:1.237936179370739\n",
      "train loss:1.078053234389724\n",
      "train loss:1.0120190921700651\n",
      "train loss:1.0054221228711522\n",
      "train loss:1.097593848045475\n",
      "train loss:1.062450339500577\n",
      "train loss:1.0455338707822963\n",
      "train loss:1.04274204349274\n",
      "train loss:1.0049468997539757\n",
      "train loss:0.9810901205538152\n",
      "train loss:0.8478004697403811\n",
      "train loss:1.0571581273041906\n",
      "train loss:1.0073046289986343\n",
      "train loss:1.061060476564995\n",
      "train loss:1.0285194535339377\n",
      "train loss:0.8878486287382166\n",
      "train loss:1.0330282292137114\n",
      "train loss:0.9889494421935623\n",
      "train loss:1.0179308443916966\n",
      "train loss:0.9448182720705143\n",
      "train loss:0.880684549399145\n",
      "train loss:1.0193122661629705\n",
      "train loss:1.0748544304415926\n",
      "train loss:1.2284017867021113\n",
      "train loss:0.8939657520561103\n",
      "train loss:0.8750695259814786\n",
      "train loss:0.9265844304686875\n",
      "train loss:0.9079418837753939\n",
      "train loss:0.9577508356316744\n",
      "train loss:0.945158058641356\n",
      "train loss:0.9588067868995593\n",
      "train loss:0.8233043794919691\n",
      "train loss:0.8552326354978189\n",
      "train loss:1.2646119972106518\n",
      "train loss:0.917064680735983\n",
      "train loss:1.1302509087086943\n",
      "train loss:1.0677375807475231\n",
      "train loss:1.1475556588451652\n",
      "train loss:1.0568980175250677\n",
      "train loss:0.9153056562813113\n",
      "train loss:0.9758704343969749\n",
      "train loss:1.0356083099531528\n",
      "train loss:1.1059824014288133\n",
      "train loss:0.9163662074757346\n",
      "train loss:0.9963364290145991\n",
      "train loss:0.9779654823730138\n",
      "train loss:0.9243798433453846\n",
      "train loss:0.9848351524490251\n",
      "train loss:1.1032747539416485\n",
      "train loss:1.3440193456145897\n",
      "train loss:1.0130098001956709\n",
      "train loss:0.9489665895868384\n",
      "train loss:0.8750903308291172\n",
      "train loss:1.1084421437130194\n",
      "train loss:0.9959219325697245\n",
      "train loss:1.0667137827378055\n",
      "train loss:0.9206277854378242\n",
      "train loss:1.0993046077175272\n",
      "train loss:0.8453428086175357\n",
      "train loss:0.9229684397249733\n",
      "train loss:1.0802431047657508\n",
      "train loss:1.0853083477572836\n",
      "train loss:0.9082210196514431\n",
      "train loss:1.1131413467903581\n",
      "train loss:1.0811500726720704\n",
      "train loss:0.9752038215248665\n",
      "train loss:0.9778882715392021\n",
      "train loss:1.1074048845419502\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9008157808603251\n",
      "train loss:1.0373640554023214\n",
      "train loss:1.0099510012023825\n",
      "train loss:0.8810555845316884\n",
      "train loss:1.1148359555931355\n",
      "train loss:1.1164781888585131\n",
      "train loss:1.007336138608155\n",
      "train loss:1.0598962435413541\n",
      "train loss:0.9663270162174159\n",
      "train loss:0.944609331544573\n",
      "train loss:0.9664255656333157\n",
      "train loss:1.0639209838334533\n",
      "train loss:0.8943882963286539\n",
      "train loss:0.8185307816682338\n",
      "train loss:0.9588669921575352\n",
      "train loss:1.1135369445901715\n",
      "train loss:0.857098955828334\n",
      "train loss:1.1251527543554953\n",
      "train loss:0.9539181248596313\n",
      "train loss:1.0088551830174675\n",
      "train loss:1.1306645751920807\n",
      "train loss:1.0101398563824244\n",
      "train loss:0.8227604500007949\n",
      "train loss:0.9271370596822497\n",
      "train loss:0.8885981697806205\n",
      "train loss:1.1549886965386658\n",
      "train loss:0.852517893168763\n",
      "train loss:1.2268916614274357\n",
      "train loss:0.9728028408676461\n",
      "train loss:0.9202241252860185\n",
      "train loss:1.1717026954634364\n",
      "train loss:1.1299274382349798\n",
      "train loss:0.8513967731139129\n",
      "train loss:1.1157081760923466\n",
      "train loss:0.9899883443818538\n",
      "train loss:0.9078690440606689\n",
      "train loss:1.0934235053501664\n",
      "train loss:0.9508817386904007\n",
      "train loss:1.208589409903512\n",
      "train loss:0.9603936022091945\n",
      "train loss:0.7730297934042953\n",
      "train loss:1.0755281477224223\n",
      "train loss:1.0823100142719009\n",
      "train loss:0.9667216452894098\n",
      "train loss:1.1491734879989246\n",
      "train loss:0.8895669668703128\n",
      "train loss:0.9590525419143175\n",
      "train loss:0.9599154954814826\n",
      "train loss:1.09672770243385\n",
      "train loss:1.003132076244673\n",
      "train loss:0.8758025000715822\n",
      "train loss:1.0003433556591017\n",
      "train loss:0.9135092204007715\n",
      "train loss:0.9610920705057218\n",
      "train loss:0.9442330414595211\n",
      "train loss:1.0717615155996467\n",
      "train loss:0.8133838424772937\n",
      "train loss:0.7936077200700019\n",
      "train loss:0.785311238673956\n",
      "train loss:1.0634839069948272\n",
      "train loss:1.0509029557119702\n",
      "train loss:0.9183808447768064\n",
      "train loss:1.0254836239087224\n",
      "train loss:1.0074168109556667\n",
      "train loss:0.9011188076888206\n",
      "train loss:1.028807735587453\n",
      "train loss:0.8554639473741759\n",
      "train loss:1.0561872578222318\n",
      "train loss:0.9908078513477402\n",
      "train loss:1.040364878468088\n",
      "train loss:0.963755884620709\n",
      "train loss:1.0694640487138716\n",
      "train loss:1.1128275259553504\n",
      "train loss:0.8941961604358031\n",
      "train loss:1.0559327656812252\n",
      "train loss:1.0041517030303722\n",
      "train loss:0.9003219567131663\n",
      "train loss:0.9286313873825754\n",
      "train loss:1.0170857391652104\n",
      "train loss:1.0689491402042774\n",
      "train loss:1.0509976559192566\n",
      "train loss:1.0867240230829494\n",
      "train loss:1.0034314729589855\n",
      "train loss:0.9980859070759496\n",
      "train loss:0.9003276811518419\n",
      "train loss:0.8728326384793335\n",
      "train loss:0.9810627092916557\n",
      "train loss:1.1128069195864736\n",
      "train loss:1.0102641479804704\n",
      "train loss:0.9735509956259577\n",
      "train loss:1.1717836063036429\n",
      "train loss:0.9248106808182891\n",
      "train loss:1.096023820972618\n",
      "train loss:0.9457492521320192\n",
      "train loss:1.0422611671063557\n",
      "train loss:0.9269898283193727\n",
      "train loss:1.0915701957737396\n",
      "train loss:1.0526531946956843\n",
      "train loss:0.9715118180018137\n",
      "train loss:0.9045549204316348\n",
      "train loss:0.919202177421914\n",
      "train loss:0.9966193754866075\n",
      "train loss:0.8660012484779704\n",
      "train loss:1.022267347140473\n",
      "train loss:0.9973564195645569\n",
      "train loss:1.0792638869506062\n",
      "train loss:1.091009981958686\n",
      "train loss:0.8549514312284107\n",
      "train loss:0.8916010633175283\n",
      "train loss:0.9925881296064294\n",
      "train loss:1.0727485716768985\n",
      "train loss:0.99257214335579\n",
      "train loss:1.034821795074822\n",
      "train loss:1.0086783922549925\n",
      "train loss:1.0010039192885496\n",
      "train loss:0.9175086478047956\n",
      "train loss:0.9616352444625567\n",
      "train loss:0.9361624922544469\n",
      "train loss:0.9382361984743315\n",
      "train loss:1.0279408776634382\n",
      "train loss:0.9965935500393678\n",
      "train loss:0.9192867237422467\n",
      "train loss:1.0640429179035997\n",
      "train loss:0.9764226893257384\n",
      "train loss:1.203781652934857\n",
      "train loss:0.7815323636803752\n",
      "train loss:0.9227545303606115\n",
      "train loss:1.0264246811482136\n",
      "train loss:1.020280880390993\n",
      "train loss:1.0351472565598854\n",
      "train loss:0.9017468096845388\n",
      "train loss:0.9767674912845041\n",
      "train loss:0.8634333595189203\n",
      "train loss:1.0177165085539464\n",
      "train loss:0.9568982968897535\n",
      "train loss:0.9292495182219419\n",
      "train loss:1.1651648811058286\n",
      "train loss:1.0198108722901205\n",
      "train loss:0.9842855112963547\n",
      "train loss:1.026903703942122\n",
      "train loss:1.0454044347559293\n",
      "train loss:0.8131923170172761\n",
      "train loss:1.2055056692327186\n",
      "train loss:1.0202561855758738\n",
      "train loss:1.0462390831508155\n",
      "train loss:1.0151564219224878\n",
      "train loss:0.9150505608017935\n",
      "train loss:1.0852680162856876\n",
      "train loss:0.851680297129663\n",
      "train loss:1.0045255036836633\n",
      "train loss:0.96176306143267\n",
      "train loss:0.9546293093802274\n",
      "train loss:0.9550473013235652\n",
      "train loss:1.0424518567713543\n",
      "train loss:1.0361538866804565\n",
      "train loss:0.9293867943937002\n",
      "train loss:1.140165811965297\n",
      "train loss:0.9543552210847774\n",
      "train loss:1.0473159063134345\n",
      "train loss:1.2242699995770285\n",
      "train loss:1.086846417838039\n",
      "train loss:1.0320798945378333\n",
      "train loss:0.9492276455734787\n",
      "train loss:1.032088289213471\n",
      "train loss:1.018978005399897\n",
      "train loss:1.0876700122079084\n",
      "train loss:0.9118643896430585\n",
      "train loss:0.9403918957941376\n",
      "train loss:1.139402168302319\n",
      "train loss:0.8073387321441658\n",
      "train loss:1.1183227441327295\n",
      "train loss:0.8175314097672477\n",
      "train loss:1.029899593147747\n",
      "train loss:0.9750812894436052\n",
      "train loss:1.0891020969592555\n",
      "train loss:0.9520128262157876\n",
      "train loss:1.119092580050995\n",
      "train loss:1.0135769264658272\n",
      "train loss:1.0086819143498844\n",
      "train loss:0.8645478372782973\n",
      "train loss:1.1265457212816588\n",
      "train loss:1.013391960505859\n",
      "train loss:1.0019452525272732\n",
      "train loss:0.9605825144336108\n",
      "train loss:0.94656924835784\n",
      "train loss:0.9910116486253266\n",
      "train loss:1.1653738771551398\n",
      "train loss:1.0911303874740972\n",
      "train loss:0.8946981714851607\n",
      "train loss:1.0005542168042956\n",
      "train loss:0.8122432039718003\n",
      "train loss:0.9653319572290135\n",
      "train loss:1.0367299370391205\n",
      "train loss:0.9816807083661397\n",
      "train loss:0.8838883992962212\n",
      "train loss:0.9846163856755381\n",
      "train loss:0.979145057745729\n",
      "train loss:1.049054283417333\n",
      "train loss:0.9870818498613629\n",
      "train loss:0.9057756401021159\n",
      "train loss:0.8743671175418357\n",
      "train loss:0.9500350672085526\n",
      "train loss:0.8908164868378051\n",
      "train loss:0.9711896395699682\n",
      "train loss:1.0070641626341583\n",
      "train loss:0.9957792939431854\n",
      "train loss:1.0018269529142059\n",
      "train loss:1.0173772087066204\n",
      "train loss:0.9305833346799518\n",
      "train loss:0.8903650235969722\n",
      "train loss:0.8808721993297728\n",
      "train loss:1.086675011798138\n",
      "train loss:0.9264901490483353\n",
      "train loss:1.0377379613225357\n",
      "train loss:1.0319156031519932\n",
      "train loss:1.1275529586473363\n",
      "train loss:1.1237902533694708\n",
      "train loss:0.8217548536374244\n",
      "train loss:0.9200649695591581\n",
      "train loss:0.9967262589406226\n",
      "train loss:0.9750969209483439\n",
      "train loss:0.9005575362703169\n",
      "train loss:1.0345683714814549\n",
      "train loss:1.0719601823152543\n",
      "train loss:0.8801621590327767\n",
      "train loss:0.8700946003231056\n",
      "train loss:0.9670297644254695\n",
      "train loss:0.9472181563042719\n",
      "train loss:1.0691813144667208\n",
      "train loss:1.0447627190865645\n",
      "train loss:0.9600973897400814\n",
      "train loss:1.0243910525320945\n",
      "train loss:0.9850611957595299\n",
      "train loss:0.9649662597746835\n",
      "train loss:1.0613364641008045\n",
      "train loss:0.8426708752330281\n",
      "train loss:0.9692532341898484\n",
      "train loss:0.7958446616622752\n",
      "train loss:0.9514611059164305\n",
      "train loss:1.0113265367562003\n",
      "train loss:1.1148967156300131\n",
      "train loss:1.0761947504714746\n",
      "train loss:0.9253396678782735\n",
      "train loss:1.1936196635653684\n",
      "train loss:0.9043710137749824\n",
      "train loss:0.9022439442183185\n",
      "train loss:0.9812106059916995\n",
      "train loss:0.9790514839259629\n",
      "train loss:0.9793678339397437\n",
      "train loss:1.023898142302392\n",
      "train loss:0.9559500514523301\n",
      "train loss:0.9424007574581748\n",
      "train loss:1.049491191096098\n",
      "train loss:1.0846819447929859\n",
      "train loss:0.9235995254606203\n",
      "train loss:0.9121996443957144\n",
      "train loss:1.1733802145441563\n",
      "train loss:1.2445624346102977\n",
      "train loss:0.8432812851807276\n",
      "train loss:0.9501800089687275\n",
      "train loss:1.110136311222736\n",
      "train loss:0.9980439734864447\n",
      "train loss:1.0277492907407464\n",
      "train loss:0.9584971363273638\n",
      "train loss:0.9988574744594725\n",
      "train loss:1.0912014980950666\n",
      "train loss:1.053955179673171\n",
      "train loss:0.9143241727962632\n",
      "train loss:1.0843389103051182\n",
      "train loss:1.043611971266576\n",
      "train loss:0.995339176776632\n",
      "train loss:0.8601310951402094\n",
      "train loss:0.9623189787976082\n",
      "train loss:1.0352467899490454\n",
      "train loss:1.0021895703603119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.973493386099909\n",
      "train loss:1.0460211031580584\n",
      "train loss:0.9707850520284328\n",
      "train loss:1.0293460334882605\n",
      "train loss:0.9658377546030061\n",
      "train loss:0.8539022246738232\n",
      "train loss:1.0096697301134083\n",
      "train loss:0.9478826544007317\n",
      "train loss:0.9103102681802848\n",
      "train loss:0.8689694983319982\n",
      "train loss:1.0819674593261956\n",
      "train loss:0.9336512304414724\n",
      "train loss:0.9816923306923677\n",
      "train loss:1.0653371377838972\n",
      "train loss:0.8555731288249436\n",
      "train loss:1.077087426361362\n",
      "train loss:1.016272872070958\n",
      "train loss:0.8563979141913368\n",
      "train loss:1.0256493269712585\n",
      "train loss:1.0924531121073189\n",
      "train loss:0.9150612262777748\n",
      "train loss:0.9239613945175311\n",
      "train loss:1.0907874999386826\n",
      "train loss:0.8460407175462896\n",
      "train loss:1.0096235620137763\n",
      "train loss:0.9120058065530641\n",
      "train loss:0.9259601878935646\n",
      "train loss:1.0243354067151114\n",
      "train loss:0.9366794224351871\n",
      "train loss:0.9999259905217273\n",
      "train loss:1.006816596491477\n",
      "train loss:0.8273472712977004\n",
      "train loss:1.021190139703394\n",
      "train loss:0.9559711100914526\n",
      "train loss:1.0050510084852062\n",
      "train loss:0.933107788170079\n",
      "train loss:1.0827271056642498\n",
      "train loss:0.8306308471526695\n",
      "train loss:0.9532475308259687\n",
      "train loss:1.1001556452922803\n",
      "train loss:0.9816205850068256\n",
      "train loss:1.007303741298722\n",
      "train loss:0.887023857809146\n",
      "train loss:1.1516859326649291\n",
      "train loss:1.0953327442376162\n",
      "train loss:1.0791203843937156\n",
      "train loss:0.9466473532727595\n",
      "train loss:1.0221649249472924\n",
      "train loss:0.9224124768664936\n",
      "train loss:0.6936096454569908\n",
      "train loss:1.0137640045634677\n",
      "train loss:1.0489121085438566\n",
      "train loss:0.9962425460735987\n",
      "train loss:1.0899061702836952\n",
      "train loss:1.012373875665087\n",
      "train loss:0.9802541554959353\n",
      "train loss:0.8785432337533409\n",
      "train loss:0.9115774978095398\n",
      "train loss:0.9317041819066035\n",
      "train loss:0.9707376834153243\n",
      "train loss:1.000336537587381\n",
      "train loss:0.8672379436303976\n",
      "train loss:1.111129752059142\n",
      "train loss:0.9058902835749935\n",
      "train loss:0.9936105090783457\n",
      "train loss:0.9309831621126823\n",
      "train loss:0.9336492501366145\n",
      "train loss:1.1680388449551151\n",
      "train loss:1.0422911762782443\n",
      "train loss:1.016237916595793\n",
      "train loss:0.9459940266923664\n",
      "train loss:1.0578787683132345\n",
      "train loss:0.9577587834795329\n",
      "train loss:1.1895377668253684\n",
      "train loss:1.012337363454625\n",
      "train loss:1.130554535396075\n",
      "train loss:0.785085218330792\n",
      "train loss:0.8035130142699408\n",
      "train loss:0.9790655870091923\n",
      "train loss:1.0380772833887015\n",
      "train loss:1.172767135752175\n",
      "train loss:0.8851112937507394\n",
      "train loss:0.927327147683694\n",
      "train loss:1.042552172271964\n",
      "train loss:1.0822947822287932\n",
      "train loss:0.9292938270713593\n",
      "train loss:1.0136427880188223\n",
      "train loss:1.0381099713499369\n",
      "train loss:0.8948721105880761\n",
      "train loss:0.8974014949911848\n",
      "train loss:1.210288963123449\n",
      "train loss:1.1138831533926161\n",
      "train loss:0.8027629913627724\n",
      "train loss:0.9930429652052517\n",
      "train loss:0.9007827224136875\n",
      "train loss:0.955902960172502\n",
      "train loss:0.9358009749792453\n",
      "train loss:0.9497364700940066\n",
      "train loss:0.9327203448140132\n",
      "train loss:1.0937276948626342\n",
      "train loss:0.9903000926380888\n",
      "train loss:0.9735552320165572\n",
      "train loss:1.0147793138721029\n",
      "train loss:0.9859184004813172\n",
      "train loss:0.9434965382993958\n",
      "train loss:0.9360450385227449\n",
      "train loss:0.8600639692034031\n",
      "train loss:0.7925125218654904\n",
      "train loss:1.0269454107104496\n",
      "train loss:0.992200988842932\n",
      "train loss:1.082571765796255\n",
      "train loss:0.9984249335090041\n",
      "train loss:1.01810002283094\n",
      "train loss:0.9048355377659014\n",
      "train loss:1.0025869977756898\n",
      "train loss:0.762235630096638\n",
      "train loss:1.0311988571528579\n",
      "=== epoch:3, train acc:0.978, test acc:0.982 ===\n",
      "train loss:0.9238111007053956\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from deep_convnet import DeepConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()  \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep_convnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "\n",
    "\n",
    "class DeepConvNet:\n",
    "    \"\"\"認識率99%以上の高精度なConvNet\n",
    "\n",
    "    ネットワーク構成は下記の通り\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout - affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        # 重みの初期化===========\n",
    "        # 各層のニューロンひとつあたりが、前層のニューロンといくつのつながりがあるか（TODO:自動で計算する）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        weight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLUを使う場合に推奨される初期値\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = weight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = weight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = weight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# half_float_network.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_convnet import DeepConvNet\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()\n",
    "network.load_params(\"deep_convnet_params.pkl\")\n",
    "\n",
    "sampled = 10000 # 高速化のため\n",
    "x_test = x_test[:sampled]\n",
    "t_test = t_test[:sampled]\n",
    "\n",
    "print(\"caluculate accuracy (float64) ... \")\n",
    "print(network.accuracy(x_test, t_test))\n",
    "\n",
    "# float16に型変換\n",
    "x_test = x_test.astype(np.float16)\n",
    "for param in network.params.values():\n",
    "    param[...] = param.astype(np.float16)\n",
    "\n",
    "print(\"caluculate accuracy (float16) ... \")\n",
    "print(network.accuracy(x_test,b t_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# misclassified_mnist.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deep_convnet import DeepConvNet\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()\n",
    "network.load_params(\"deep_convnet_params.pkl\")\n",
    "\n",
    "print(\"calculating test accuracy ... \")\n",
    "#sampled = 1000\n",
    "#x_test = x_test[:sampled]\n",
    "#t_test = t_test[:sampled]\n",
    "\n",
    "classified_ids = []\n",
    "\n",
    "acc = 0.0\n",
    "batch_size = 100\n",
    "\n",
    "for i in range(int(x_test.shape[0] / batch_size)):\n",
    "    tx = x_test[i*batch_size:(i+1)*batch_size]\n",
    "    tt = t_test[i*batch_size:(i+1)*batch_size]\n",
    "    y = network.predict(tx, train_flg=False)\n",
    "    y = np.argmax(y, axis=1)\n",
    "    classified_ids.append(y)\n",
    "    acc += np.sum(y == tt)\n",
    "    \n",
    "acc = acc / x_test.shape[0]\n",
    "print(\"test accuracy:\" + str(acc))\n",
    "\n",
    "classified_ids = np.array(classified_ids)\n",
    "classified_ids = classified_ids.flatten()\n",
    " \n",
    "max_view = 20\n",
    "current_view = 1\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.2, wspace=0.2)\n",
    "\n",
    "mis_pairs = {}\n",
    "for i, val in enumerate(classified_ids == t_test):\n",
    "    if not val:\n",
    "        ax = fig.add_subplot(4, 5, current_view, xticks=[], yticks=[])\n",
    "        ax.imshow(x_test[i].reshape(28, 28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        mis_pairs[current_view] = (t_test[i], classified_ids[i])\n",
    "            \n",
    "        current_view += 1\n",
    "        if current_view > max_view:\n",
    "            break\n",
    "\n",
    "print(\"======= misclassified result =======\")\n",
    "print(\"{view index: (label, inference), ...}\")\n",
    "print(mis_pairs)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
